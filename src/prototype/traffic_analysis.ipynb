{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contained-mongolia",
   "metadata": {},
   "source": [
    "export DISPLAY=`grep -oP \"(?<=nameserver ).+\" /etc/resolv.conf`:0.0\n",
    "\n",
    "must be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "essential-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "import xlsxwriter\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from sort import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surgical-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "VIDEO = 'full_360p_noAudio_24fps.mov'\n",
    "FPS = 24\n",
    "VIDEO_WIDTH = 640\n",
    "VIDEO_HEIGHT = 360\n",
    "CLASSES = {'bicycle', 'car', 'motorcycle', 'bus', 'truck'}\n",
    "TRACK_HISTORY = 50\n",
    "\n",
    "# more models can be found here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n",
    "# faster model. not as effective\n",
    "MODEL_DATE = '20200713'\n",
    "MODEL_NAME = 'centernet_hg104_512x512_coco17_tpu-8'\n",
    "\n",
    "# slow model. Effective\n",
    "#MODEL_DATE = '20200711'\n",
    "#MODEL_NAME = 'efficientdet_d7_coco17_tpu-32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rental-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "MODELS_DIR = os.path.join(DATA_DIR, 'models')\n",
    "for dir in [DATA_DIR, MODELS_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "        \n",
    "if os.path.exists('objects'):\n",
    "    shutil.rmtree('objects')\n",
    "os.mkdir('objects')\n",
    "\n",
    "if not os.path.exists('statistics'):\n",
    "    os.mkdir('statistics')\n",
    "    \n",
    "if not os.path.exists('output_videos'):\n",
    "    os.mkdir('output_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pleasant-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model\n",
    "MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'\n",
    "MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME\n",
    "PATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)\n",
    "PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))\n",
    "PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))\n",
    "if not os.path.exists(PATH_TO_CKPT):\n",
    "    print('Downloading model. This may take a while... ', end='')\n",
    "    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)\n",
    "    tar_file = tarfile.open(PATH_TO_MODEL_TAR)\n",
    "    tar_file.extractall(MODELS_DIR)\n",
    "    tar_file.close()\n",
    "    os.remove(PATH_TO_MODEL_TAR)\n",
    "    print('Done')\n",
    "\n",
    "# Download labels file\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "LABELS_DOWNLOAD_BASE = \\\n",
    "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))\n",
    "if not os.path.exists(PATH_TO_LABELS):\n",
    "    print('Downloading label file... ', end='')\n",
    "    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "photographic-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    \n",
    "\n",
    "# Suppress TensorFlow logging (2)\n",
    "tf.get_logger().setLevel('ERROR')           \n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "    \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def detect_fn(image):\n",
    "        \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "        image, shapes = model.preprocess(image)\n",
    "        prediction_dict = model.predict(image, shapes)\n",
    "        detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "        return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "    return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleasant-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load categories\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sensitive-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_and_time():\n",
    "    \"\"\" Returns current date and time.\n",
    "    \n",
    "    This function is used to add the date and time to the output video and excel file.\n",
    "    \n",
    "    Return value:\n",
    "    dt_string -- date and time\n",
    "    \"\"\"\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    \n",
    "    return dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "leading-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(detections, wanted_classes, category_index, min_thresh=0.3):\n",
    "    \"\"\" Return only the detections for the wanted classes and above the minimum threshold.\n",
    "    \n",
    "    This function is used to filter the detections from the Tensorflow object detection api.\n",
    "    The detections are filtered by the minimum threshold as well by the wanted classes.\n",
    "    \n",
    "    In this project it is used for the displaying and tracking of the wanted vehicle types.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    detections -- detections by the Tensorflow object detection api\n",
    "    wanted_classes -- Dict, with all wanted classes who will be displayed and tracked\n",
    "    category_index -- category index by the model\n",
    "    min_thresh -- minimum detection score of detections which should be shown\n",
    "    \n",
    "    Return value:\n",
    "    wanted_detections -- same as detections but the filtered ones\n",
    "    \"\"\"\n",
    "\n",
    "    # extract values from the detections array\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    classes = detections['detection_classes'][0].numpy()\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    \n",
    "    # get class ids by class name\n",
    "    class_ids = []\n",
    "    for value in category_index.values():\n",
    "        name = value.get('name')\n",
    "        id = value.get('id')\n",
    "        if name in wanted_classes:\n",
    "            class_ids.append(id - 1)\n",
    "    \n",
    "    # get detected objects with min. thresh and wanted class id\n",
    "    wanted_boxes = []\n",
    "    wanted_scores = []\n",
    "    wanted_classes = []\n",
    "    for i in range(classes.size):\n",
    "        if classes[i] in class_ids and scores[i] > min_thresh:\n",
    "            wanted_boxes.append(boxes[i])\n",
    "            wanted_classes.append(classes[i])\n",
    "            wanted_scores.append(scores[i])\n",
    "    \n",
    "    # prepare data as tensors\n",
    "    wanted_detections = {\n",
    "        'detection_boxes': tf.expand_dims(tf.convert_to_tensor(wanted_boxes), axis=0),\n",
    "        'detection_classes': tf.expand_dims(tf.convert_to_tensor(wanted_classes), axis=0),\n",
    "        'detection_scores': tf.expand_dims(tf.convert_to_tensor(wanted_scores), axis=0)\n",
    "    }\n",
    "    \n",
    "    return wanted_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wooden-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(frame):\n",
    "    \"\"\" Returns detections for given frame\n",
    "    \n",
    "    This function is used to run the object detection for every frame.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    frame -- given frame to apply the object detection\n",
    "    \n",
    "    Return value:\n",
    "    detections -- includes detection boxes, scores, classes, ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(frame, axis=0)\n",
    "        \n",
    "    input_tensor = tf.convert_to_tensor(image_np_expanded, dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "novel-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_arrays(history, screenshots, direction_tracking, speeds, speed_tracking, track_id_score, track_id_class, stats, directions):\n",
    "    \"\"\" Cleans up all arrays and dicts each frame.\n",
    "    \n",
    "    In order not to have too much temporary memory, each frame only the last TRACK_HISTORY track_id's are stored.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    All arrays and dicts who will be cleaned up.\n",
    "    \"\"\"\n",
    "    \n",
    "    # clean up screenshots\n",
    "    screenshots = [id for id in screenshots if id >= track_id - history]\n",
    "                    \n",
    "    # clean up direction_tracking\n",
    "    for key in list(direction_tracking.keys()):\n",
    "        if key <= track_id - history:\n",
    "            direction_tracking.pop(key, None)\n",
    "                            \n",
    "    # clean up speeds\n",
    "    for key in list(speeds.keys()):\n",
    "        if key <= track_id - history:\n",
    "            speeds.pop(key, None)\n",
    "                            \n",
    "    # clean up speed_tracking\n",
    "    for key in list(speed_tracking.keys()):\n",
    "        if key <= track_id - history:\n",
    "            speed_tracking.pop(key, None)\n",
    "            \n",
    "    # clean up track_id_score\n",
    "    for key in list(track_id_score.keys()):\n",
    "        if key <= track_id - history:\n",
    "            track_id_score.pop(key, None)\n",
    "            \n",
    "    # clean up track_id_class\n",
    "    for key in list(track_id_class.keys()):\n",
    "        if key <= track_id - history:\n",
    "            track_id_class.pop(key, None)\n",
    "            \n",
    "    # clean up stats\n",
    "    stats = [id for id in stats if id >= track_id - history]\n",
    "    \n",
    "    # clean up directions\n",
    "    for key in list(directions.keys()):\n",
    "        if key <= track_id - history:\n",
    "            directions.pop(key, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dedicated-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_calculation(locations, speeds):\n",
    "    \"\"\" Calculation the speed of an object.\n",
    "    \n",
    "    This function is used to calculate and save the speed of an object in km/h.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    locations -- x coords of the given object\n",
    "    speeds -- speeds dict to save the speed by track_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # Only objects that will drive completely through the speed detection area will be calculated\n",
    "    if (locations[-1] >= 452 and min(locations) < 308) or (locations[-1] <= 308 and max(locations) > 452):\n",
    "        # get all x coordinates in the speed detection area\n",
    "        speed_tracking[track_id] = len([mark for mark in locations if (mark >= 308 and mark <=452)])\n",
    "                                \n",
    "        if speed_tracking[track_id] != 0:\n",
    "            # speed = 4.5 meters / (amount of x coordinates in area / fps) * 3.6\n",
    "            speed = 4.5/(speed_tracking[track_id]/FPS)*3.6\n",
    "            \n",
    "            # false calculation\n",
    "            if speed < 100.0:\n",
    "                speeds[track_id] = speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "valued-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_directions(frame, locations, rectangle, directions, track_id):\n",
    "    \"\"\" Display the directions with an arrow.\n",
    "    \n",
    "    To display the directions of an object it will check the last three locations of it.\n",
    "    It is displayed as an arrow on the right or left side of the objects detection box.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    frame -- frame where the direction should be displayed\n",
    "    locations -- x coords of the object\n",
    "    rectangle -- rectangle of the object\n",
    "    directions -- directions dict to save the directions of the object\n",
    "    track_id -- current object\n",
    "    \n",
    "    Return value:\n",
    "    frame -- frame with directions\n",
    "    \"\"\"\n",
    "    x1, x2, y1, y2 = rectangle\n",
    "    \n",
    "    # get height of arrow depending on the rectangle size\n",
    "    y_startPoint = int(y1+((y2-y1)/2))\n",
    "                        \n",
    "    # object moves to the right\n",
    "    if locations[-1] > (locations[-2] and  locations[-3]):\n",
    "        cv2.arrowedLine(frame, (x2+5, y_startPoint), (x2+25, y_startPoint), (0,255,0), thickness=1, tipLength=0.2)\n",
    "        directions[track_id] = 'right'\n",
    "                            \n",
    "    # object moves to the left\n",
    "    elif locations[-1] < (locations[-2] and locations[-3]): # left\n",
    "        cv2.arrowedLine(frame, (x1-5, y_startPoint), (x1-25, y_startPoint), (0,255,0), thickness=1, tipLength=0.2)\n",
    "        directions[track_id] = 'left'\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordinary-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_speeds(frame, track_id, speeds, rectangle):\n",
    "    \"\"\" Display the speed of an object.\n",
    "    \n",
    "    The speed will be display under the object as soon as the speed is calculated.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    frame -- frame where the speed should be displayed\n",
    "    track_id -- current object\n",
    "    speeds -- speeds dict to get the speed\n",
    "    rectangle -- rectangle of the object\n",
    "    \n",
    "    Return value:\n",
    "    frame -- frame with speeds\n",
    "    \"\"\"\n",
    "    \n",
    "    x1, x2, y1, y2 = rectangle\n",
    "    \n",
    "    # display speed below the track rectangle\n",
    "    if track_id in speeds:\n",
    "        text = \"{0:.1f}km/h\".format(speeds[track_id])\n",
    "        (speedTextSize_width, speedTextSize_height), baseline_speed = cv2.getTextSize(text, font, fontScale, lineType)\n",
    "        speedTextSize = (speedTextSize_width, speedTextSize_height)\n",
    "        speedTextX = int(x1+(x2-x1)/2 - (speedTextSize[0] / 2))\n",
    "        speedTextY = int(y2 + (speedTextSize[1] / 2) + speedTextSize_height/2 + 7)\n",
    "        bottomLeftCornerOfSpeedText = (speedTextX, speedTextY)\n",
    "                        \n",
    "        cv2.rectangle(frame, (speedTextX - baseline_speed, speedTextY - speedTextSize[1] - baseline_speed), (speedTextX + speedTextSize[0] + baseline_speed, speedTextY + baseline_speed), (0,255,0), thickness=-1)\n",
    "        cv2.putText(frame, text, (bottomLeftCornerOfSpeedText), font, fontScale, fontColor, lineType)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assured-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tracking_detection(frame, x_coord, rectangle, track_id_score, track_id_class, track_id):\n",
    "    \"\"\" Display data by the object detection.\n",
    "    \n",
    "    Display the score and class of an object by the object detection. It is displayed above the rectangle by the tracking.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    frame -- frame where the object detection should be displayed\n",
    "    x_coord -- used to shoose color of rectangle\n",
    "    rectangle -- rectangle of tracking\n",
    "    track_id_score -- scores for each track_id\n",
    "    track_id_class -- classes for each track_id\n",
    "    track_id -- current track_id\n",
    "    \n",
    "    Return value:\n",
    "    frame -- frame with scores and classes\n",
    "    \"\"\"\n",
    "    \n",
    "    x1, x2, y1, y2 = rectangle\n",
    "    \n",
    "    # display score and class\n",
    "    # get middle of top rectangle\n",
    "    middle = (int(x1 + ((x2-x1)/2)), y1)\n",
    "    \n",
    "    score_text = '{}%'.format(track_id_score[track_id])\n",
    "    class_text = track_id_class[track_id]\n",
    "        \n",
    "    # align score text\n",
    "    (scoreTextSize_width, scoreTextSize_height), baseline_score = cv2.getTextSize(score_text, font, fontScale, lineType)\n",
    "    scoreTextSize = (scoreTextSize_width, scoreTextSize_height)\n",
    "    scoreTextX = int(middle[0] - (scoreTextSize[0] / 2))\n",
    "    scoreTextY = int(middle[1] + (scoreTextSize[1] / 2) -12)\n",
    "    bottomLeftCornerOfScoreText = (scoreTextX, scoreTextY)\n",
    "        \n",
    "    # align class name text\n",
    "    (classTextSize_width, classTextSize_height), baseline_class = cv2.getTextSize(class_text, font, fontScale, lineType)\n",
    "    classTextSize = (classTextSize_width, classTextSize_height)\n",
    "    classTextX = int(middle[0] - (classTextSize[0] / 2))\n",
    "    classTextY = int(middle[1] + (classTextSize[1] / 2) -30)\n",
    "    bottomLeftCornerOfClassText = (classTextX, classTextY)\n",
    "        \n",
    "    # display score\n",
    "    cv2.rectangle(frame, (scoreTextX - baseline_score, scoreTextY - scoreTextSize[1] - baseline_score), (scoreTextX + scoreTextSize[0] + baseline_score, scoreTextY + baseline_score), (0,255,0), thickness=-1)\n",
    "    cv2.putText(frame, score_text, bottomLeftCornerOfScoreText, font, fontScale, fontColor, lineType)\n",
    "        \n",
    "    # display class name\n",
    "    cv2.rectangle(frame, (classTextX - baseline_class, classTextY - classTextSize[1] - baseline_class), (classTextX + classTextSize[0] + baseline_class, classTextY + baseline_class), (0,255,0), thickness=-1)\n",
    "    cv2.putText(frame, class_text, bottomLeftCornerOfClassText, font, fontScale, fontColor, lineType)\n",
    "    \n",
    "    # display tracking rectangle. Red if it is in the speed detection area\n",
    "    if x_coord >= 308 and x_coord <= 452:\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,0,255), thickness=1)\n",
    "    else:\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), thickness=1)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intelligent-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(a, b):\n",
    "    \"\"\" Calculating the overlapping area of two rectangles.\n",
    "    \n",
    "    This function is used to calculate the overlapping area of two rectangles.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    a -- first rectangle\n",
    "    b -- second rectangle\n",
    "    \n",
    "    Return value:\n",
    "    area -- Area of the two overlapping rectangles\n",
    "    \"\"\"\n",
    "    \n",
    "    ax1, ax2, ay1, ay2 = a\n",
    "    bx1, bx2, by1, by2 = b\n",
    "    \n",
    "    if ax1 >= ax2:\n",
    "        axmax = ax1\n",
    "        axmin = ax2\n",
    "    else:\n",
    "        axmax = ax2\n",
    "        axmin = ax1\n",
    "        \n",
    "    if bx1 >= bx2:\n",
    "        bxmax = bx1\n",
    "        bxmin = bx2\n",
    "    else:\n",
    "        bxmax = bx2\n",
    "        bxmin = bx1\n",
    "    \n",
    "    if ay1 >= ay2:\n",
    "        aymax = ay1\n",
    "        aymin = ay2\n",
    "    else:\n",
    "        aymax = ay2\n",
    "        aymin = ay1\n",
    "        \n",
    "    if by1 >= by2:\n",
    "        bymax = by1\n",
    "        bymin = by2\n",
    "    else:\n",
    "        bymax = by2\n",
    "        bymin = by1\n",
    "    \n",
    "    dx = min(axmax, bxmax) - max(axmin, bxmin)\n",
    "    dy = min(aymax, bymax) - max(aymin, bymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informational-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movement_detection(frame, fgbg, gaussian_kernel=(7,7), dilation_kernel=(11,11), minContourArea=1200):\n",
    "    \"\"\" Movement detection of objects of a certain size.\n",
    "    \n",
    "    This function detects any kind of movement in a frame. Only movement of a certain size will be detected.\n",
    "    Only the moving objects will be shown and the rest of the image will be black.\n",
    "    \n",
    "    In this project it is used to get all moving vehicles and objects of a\n",
    "    video to differentiate between moving and standing vehicles and objects.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    frame -- frame to apply the movement detection\n",
    "    fgbg -- foreground/background subtractor\n",
    "    gaussian_kernel -- kernel size for the gaussian blur (default (7,7))\n",
    "    dilation_kernel -- kernel size for the dilation (default (11,11))\n",
    "    minContourArea -- minimum of contour area to be shown (default 1200)\n",
    "    \n",
    "    Return value:\n",
    "    movement_frame -- same as frame, but all spots without moving objects are blacked out\n",
    "    \"\"\"\n",
    "    \n",
    "    # grayscale\n",
    "    gray_frame = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # blurring with gaussian blur\n",
    "    blur_frame = cv2.GaussianBlur(src=gray_frame, ksize=gaussian_kernel, sigmaX=0)\n",
    "    \n",
    "    # applying background subtraction\n",
    "    fgmask = fgbg.apply(blur_frame)\n",
    "    \n",
    "    # dilation of the foreground mask\n",
    "    dilated_frame = cv2.dilate(src=fgmask, kernel=np.ones(dilation_kernel,np.uint8), iterations=3)\n",
    "    \n",
    "    # returning all contours\n",
    "    contours, _ = cv2.findContours(image=dilated_frame, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # create mask with same size and shape as the frame\n",
    "    mask = np.zeros_like(frame)\n",
    "    \n",
    "    # Draw rectangles around contours\n",
    "    for contour in contours:\n",
    "            \n",
    "        # Skipping small contours\n",
    "        if cv2.contourArea(contour) < minContourArea:\n",
    "            continue\n",
    "                \n",
    "        # Returning corners of the contour. Doesn't consider the rotation of the object.\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        # draw white filled contour on the mask\n",
    "        cv2.rectangle(img=mask, pt1=(x, y), pt2=(x+w, y+h), color=(255,255,255), thickness=-1)\n",
    "\n",
    "    # add mask and frame. Only show white areas from mask on the frame.\n",
    "    movement_frame = cv2.bitwise_and(frame, mask)\n",
    "    \n",
    "    return movement_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surrounded-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_track_with_object(filtered_detections, track_id):\n",
    "    \"\"\" Assign the score and class to tracked object.\n",
    "    \n",
    "    By calculating the area of both rectangle. The rectangles with the biggest area are the same object.\n",
    "    \n",
    "    Keywords arguments:\n",
    "    filtered_detections -- detections to compare\n",
    "    track_id -- current track_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # assing tracking objects with object detection objects\n",
    "    boxes = filtered_detections['detection_boxes'][0].numpy()\n",
    "    areas = []\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        # get rectangle coordinates\n",
    "        bx1 = int(VIDEO_WIDTH * boxes[i][1])\n",
    "        by1 = int(VIDEO_HEIGHT * boxes[i][0])\n",
    "        bx2 = int(VIDEO_WIDTH * boxes[i][3])\n",
    "        by2 = int(VIDEO_HEIGHT * boxes[i][2])\n",
    "        rectangle2 = (bx1, bx2, by1, by2)\n",
    "                        \n",
    "        areas.append(area(rectangle, rectangle2))\n",
    "                        \n",
    "    i = areas.index(max(areas))\n",
    "                                        \n",
    "    scores = filtered_detections['detection_scores'][0].numpy()\n",
    "    classes = filtered_detections['detection_classes'][0].numpy()\n",
    "                    \n",
    "    track_id_score[track_id] = int(scores[i] * 100)\n",
    "    track_id_class[track_id] = category_index[classes[i] + 1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlike-huntington",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get time and date\n",
    "dt_string = get_date_and_time()\n",
    "\n",
    "# load video\n",
    "video = cv2.VideoCapture(VIDEO)\n",
    "\n",
    "# open video to save the output\n",
    "result = cv2.VideoWriter('output_videos/detection_{}.mp4'.format(dt_string), cv2.VideoWriter_fourcc(*'mp4v'), 24.0, (640, 360), True)\n",
    "\n",
    "# create background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=800, detectShadows=False, varThreshold=100)\n",
    "\n",
    "# create tracker\n",
    "mot_tracker = Sort()\n",
    "\n",
    "# create temporary dicts and lists\n",
    "direction_tracking = {}\n",
    "speed_tracking = {}\n",
    "screenshots = []\n",
    "speeds = {}\n",
    "track_id_score = {}\n",
    "track_id_class = {}\n",
    "stats = []\n",
    "directions = {}\n",
    "row = 1\n",
    "amount = {'bicycle': 0, 'car': 0, 'motorcycle': 0, 'bus': 0, 'truck': 0}\n",
    "\n",
    "# Create excel file\n",
    "workbook = xlsxwriter.Workbook('statistics/stats_{}.xlsx'.format(dt_string))\n",
    "worksheet_objects = workbook.add_worksheet('Objects')\n",
    "worksheet_amount = workbook.add_worksheet('Amount')\n",
    "bold = workbook.add_format({'bold': True})\n",
    "\n",
    "# add header to Objects worksheet\n",
    "worksheet_objects.write(0,0, 'track_id', bold)\n",
    "worksheet_objects.write(0,1, 'class', bold)\n",
    "worksheet_objects.write(0,2, 'speed in km/h', bold)\n",
    "worksheet_objects.write(0,3, 'direction', bold)\n",
    "\n",
    "# define font\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.4\n",
    "fontColor = (0,0,0)\n",
    "lineType = 1\n",
    "    \n",
    "while(video.isOpened()):\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        # get movement frame\n",
    "        movement_frame = movement_detection(frame, fgbg)\n",
    "        \n",
    "        # get detections\n",
    "        detections = object_detection(movement_frame)\n",
    "        \n",
    "        # get relevant detections\n",
    "        filtered_detections = filter_detections(detections, {'bicycle', 'car', 'motorcycle', 'bus', 'truck'}, category_index, 0.4)\n",
    "        \n",
    "        # check if relevant detections are found\n",
    "        if filtered_detections['detection_boxes'].shape != (1, 0):\n",
    "            \n",
    "            # update tracker\n",
    "            track_bbs_ids = mot_tracker.update(filtered_detections['detection_boxes'][0])\n",
    "            \n",
    "            # check if tracker updated\n",
    "            if track_bbs_ids.size != 0:\n",
    "                \n",
    "                for track in track_bbs_ids:\n",
    "                    \n",
    "                    # extract values from tracking list\n",
    "                    track_id = int(track[4])\n",
    "                    x1 = int(VIDEO_WIDTH*track[1])\n",
    "                    y1 = int(VIDEO_HEIGHT*track[0])\n",
    "                    x2 = int(VIDEO_WIDTH*track[3])\n",
    "                    y2 = int(VIDEO_HEIGHT*track[2])\n",
    "                    middle = (int(x1 + ((x2-x1)/2)), int(y1 + ((y2-y1)/2)))\n",
    "                    rectangle = (x1, x2, y1, y2)\n",
    "                    x_coord = middle[0]\n",
    "                    \n",
    "                    assign_track_with_object(filtered_detections, track_id)\n",
    "                    \n",
    "                    # add x coordinates of tracked object to track_id\n",
    "                    if track_id in direction_tracking:\n",
    "                        direction_tracking[track_id].append(x_coord)\n",
    "                    else:\n",
    "                        direction_tracking[track_id] = [x_coord]\n",
    "                        \n",
    "                    # objects has to be tracked at least three times to analyze direction and speed\n",
    "                    if direction_tracking.get(track_id) != None and len(direction_tracking.get(track_id)) >= 3:\n",
    "                        \n",
    "                        # get all x coordinates of tracked object\n",
    "                        locations = direction_tracking.get(track_id)\n",
    "                        \n",
    "                        frame = display_directions(frame, locations, rectangle, directions, track_id)\n",
    "                                        \n",
    "                        speed_calculation(locations, speeds)\n",
    "\n",
    "                    display_speeds(frame, track_id, speeds, rectangle)    \n",
    "                    \n",
    "                    display_tracking_detection(frame, x_coord, rectangle, track_id_score, track_id_class, track_id)\n",
    "\n",
    "                    # save data for current frame\n",
    "                    if track_id in directions:\n",
    "                        if ((x_coord > 452 and directions[track_id] == 'right') or (x_coord < 308 and directions[track_id] == 'left')) and track_id not in stats:\n",
    "                            stats.append(track_id)\n",
    "                            if track_id in speeds:\n",
    "                                cv2.imwrite('objects/object_{}.jpg'.format(track_id), movement_frame[y1:y2, x1:x2])\n",
    "                                worksheet_objects.write(row, 0, track_id)\n",
    "                                worksheet_objects.write(row, 1, track_id_class[track_id])\n",
    "                                worksheet_objects.write_number(row, 2, speeds[track_id])\n",
    "                                worksheet_objects.write(row, 3, directions[track_id])\n",
    "                                row += 1\n",
    "                                amount[track_id_class[track_id]] += 1\n",
    "                            \n",
    "                    cleanup_arrays(TRACK_HISTORY, screenshots, direction_tracking, speeds, speed_tracking, track_id_score, track_id_class, stats, directions)\n",
    "\n",
    "        # marking speed detection area\n",
    "        cv2.line(frame, (308, 0), (308, VIDEO_HEIGHT), (255, 102, 0), thickness=1)\n",
    "        cv2.line(frame, (452, 0), (452, VIDEO_HEIGHT), (255, 102, 0), thickness=1)\n",
    "        \n",
    "        # display amount of classes\n",
    "        increaser = 0\n",
    "        for key, value in amount.items():\n",
    "            cv2.putText(frame, key + ': ' + str(value), (0,355 + increaser), font, fontScale, (0,255,0), lineType)\n",
    "            increaser -= 10\n",
    "        \n",
    "        # display frame\n",
    "        cv2.imshow('frame', cv2.resize(frame, (1920, 1080)))\n",
    "        result.write(frame)\n",
    "        \n",
    "        # Press q to exit video\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# save amount\n",
    "count = 1\n",
    "worksheet_amount.write(0, 0, 'object', bold)\n",
    "worksheet_amount.write(0, 1, 'amount', bold)\n",
    "for key, value in amount.items():\n",
    "    worksheet_amount.write(count, 0, key)\n",
    "    worksheet_amount.write_number(count, 1, value)\n",
    "    count += 1\n",
    "\n",
    "workbook.close()\n",
    "video.release()\n",
    "result.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
