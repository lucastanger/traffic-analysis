%!TEX root = ../dokumentation.tex

\chapter{Prototypische Implementierung}\label{sec:prototypical_implementation}

Mit diesem Prototyp soll ein Programm entwickelt werden, das mehrere Objekte in einer Videosequenz erkennt, deren Richtung und Geschwindigkeit misst und daraus Statistiken generiert. Verschiedene Verkehrsteilnehmer wie Fahrräder, Motorräder, Autos, LKWs und Busse sollen erkannt werden. Parkende Autos auf dem Standstreifen sollen von fahrenden unterschieden werden. Mehrere Fahrzeuge, die gleichzeitig in entgegengesetzte Richtungen fahren, sollen differenziert und vermessen werden. Es soll mit verschiedenen Blickwinkeln und Lichtverhältnissen gearbeitet werden können. Der Prototyp wurde mit den Tools OpenCV und Tensorflow 2 entwickelt und nutzt die Tensorflow Object Detection API.

\section{Bewegungserkennung}

Das Ziel der Bewegungserkennung ist es, nur sich bewegende Fahrzeuge zu analysieren und zu erkennen. Dieses Vorgehen wird mit Hilfe der OpenCV Hintergrundsubtraktion ermöglicht. Die Hintergrundsubtraktion (engl. \emph{background subtraction}) ist eine gängige und weit verbreitete Vorgehensweise zur Erzeugung einer Vordergrundmaske unter Verwendung statischer Kameras. Wie der Name besagt, berechnet die Hintergrundsubtraktion die Vordergrundmaske durch die Subtraktion zwischen dem aktuellen Bild und einem Hintergrundmodell, das den statischen Teil der Szene enthält, oder allgemeiner all das, was auf Grund der Eigenschaften der beobachteten Szene als Hintergrund betrachtet werden kann. 



Im ersten Schritt wird ein anfängliches Modell des Hintergrundes berechnet, während im nächsten Schritt dieses Modell aktualisiert wird, um sich an mögliche Änderugungen in der Szene anzupassen. Der Schwellenwert (engl. \emph{threshold}) legt fest, was als Vordergrund anerkannt wird. Resultierend aus der Berechnung entsteht eine Vordergrundmaske. 

In diesem Prototypen wird eine Hintergrundsubtraktion mit dem \ac{MOG}2 Algorithmus durchgeführt. Dabei handelt es sich um einen Mixture-basierten Hintergrund-/Vordergrund Segmentierungsalgorithmus. \citeauthor{ZIVKOVIC2006773} führt diesen Algorithmus in den Arbeiten \citetitle{1333992} und \citetitle{ZIVKOVIC2006773} an \cites{1333992}{ZIVKOVIC2006773}. 

\newpage

Eine wichtige Eigenschaft dieses Algorithmus ist die geeignete Anzahl von Gaußverteilungen für jeden Pixel. Er bietet somit eine bessere Anpassungsfähigkeit an wechselnde Szenen auf Grund von Beleuchtungsänderungen \cite{opencv_bg_subtraction2}. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/background_subtraction}
	\caption[Vorgehen der Hintergrundsubtraktion]{Vorgehen der Hintergrundsubtraktion \cite{opencv_bg_subtraction}}
\end{figure}

\vspace*{10mm}
\begin{lstlisting}[caption={Generation der Hintergrundsubtraktion}, label={lst:bgsub}]
# Generate background subtraction with opencv built-in methods    
fgbg = cv2.createBackgroundSubtractorMOG2(history=800, detectShadows=False, varThreshold=100)
\end{lstlisting}

Abbildung \ref{lst:bgsub} zeigt die Erstellung eines Hintergrundsubtraktionsobjekts für den Prototypen. Der Parameter \emph{history} wird dazu verwendet, um den Wert der Historie begrenzen zu können. Je höher dieser Wert ist, desto länger ist der Schweif den ein Objekt mit sich zieht. Für diesen Prototypen wurde die standardmäßige Schattenerkennung mit dem Parameter \emph{detectShadows} ausgeschaltet. Das Deaktivieren dieses Parameters führt zu einer deutlichen Leistungssteigerung des Algorithmus. \emph{varThreshold} ist ein Schwellenwert für den quadrierten Mahalanobis-Abstand zwischen dem Pixel und dem Modell. Dieser wird dazu verwendet, um zu entscheiden, ob ein Pixel durch das Hintergrundmodell gut beschrieben ist \cite{opencv_bg_subtraction3}. Bevor die Hintergrundsubktration auf das Video angewandt wird, muss das Video durch andere Hilfsfunktionen bearbeitet werden. Der erste Schritt zur genaueren Vorhersage ist die Grauskalierung \cite{cv_opencv} (siehe Abbildung \ref{fig:grayscale}).  

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/abbildung_grayscale}
	\caption{Graustufen des Bildes}
	\label{fig:grayscale}
\end{figure}

Eine Grauskalierung wird angewandt, da für die Bewegungserkennung keine Farbe benötigt wird und es zur Verbesserung der Anzeigequalität eines Bildes ohnehin zu einer Verminderung des Rauschens führt.

\vspace*{5mm}
\begin{lstlisting}[caption={Graustufen auf das Video anwenden}, label={lst:grayscaling}]
# Apply grayscaling on video input
gray_frame = cv2.cvtColor(src=frame, code=cv2.COLOR_BGR2GRAY)
\end{lstlisting}

Mit Hilfe der in Abbildung \ref{lst:grayscaling} aufgeführten Funktion \emph{cvtColor} wird eine Grauskalierung des aktuellen Bildes durchgeführt. Der Parameter \emph{src} stellt das aktuelle Bild dar, auf welchem die Farbkonvertierung angewendet werden soll. \emph{code} übergibt eine OpenCV Konstante, die ein Color Space Conversion Code ist. Dieser Code gibt an, ob und was für eine Art der Farbtransformation durchgeführt werden soll. Die Konstante \emph{COLOR\_BGR2GRAY} stellt die Gewichtung der Veränderung von Farbkanälen dar (siehe Abbildung \ref{eq:rgbatogray} \cite{opencv_bg_subtraction4}). 

\begin{figure}[htb]
	\[\text{RGB[A] to Gray}: Y \leftarrow 0.299 * R + 0.587 * G + 0.114 * B \]
	\caption{Gewichtung der Farbkanäle}
	\label{eq:rgbatogray}
\end{figure}

Durch den in Abbildung \ref{eq:rgbatogray} aufgezeigten Vorgang wird die Multiplikation der einzelnen Farbkanäle dargestellt. Durch das multiplizieren der Kanäle mit der jeweiligen Konstante entsteht ein neuer RGB Wert $Y$. Dieser neue Wert kann im Anschluss auf das Bild angewandt werden, um eine Graustufierung zu ermöglichen \cite{opencv_bg_subtraction4} (siehe Abbildung \ref{fig:blur}). Weitergehend wird auf das daraus entstandene Bild eine Gauß'sche Unschärfe eingesetzt. Da auch bei zwei unterschiedlichen, in sehr kurzem Abstand aufgenommenen Bildern, ein Unterschied der Beleuchtung oder dem Sensor der Kamera festgestellt werden kann, ist die Verwendung dieser Unschärfeapplikation elementar. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/abbildung_blur}
	\caption{Weichzeichnen des Bildes}
	\label{fig:blur}
\end{figure}

\vspace*{10mm}
\begin{lstlisting}[caption={Anwendung des Weichzeichners}, label={lst:gaussianblur}]
# Application of Gaussian Blur
blur_frame = cv2.GaussianBlur(src=gray_frame, ksize=(3, 3), sigmaX=0)
\end{lstlisting}

Die Gauß'sche Unschärfe ist als ein $NxN$-Tap-Faltungsfilter definiert, der die Pixel innerhalb seines Footprints, basierend auf der Gauß-Funktion gewichtet. Die Unschärfefunktion ist definiert durch 

\begin{figure}[htb]
	\[G(x,y) = \frac{1}{2\pi \sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}.\]
	\caption{Gauß'sche Unschärfefunktion}
	\label{eq:gaussian}
\end{figure}

Die Pixel des Filter-Footprints werden mit den Werten aus der Gauß-Funktion gewichtet und erzeugen so einen Unschärfe-Effekt. Die räumliche Darstellung des Gauß-Filters, zeitweise auch als \"{}Glockenfläche\"{} bezeichnet, zeigt, wie sehr die einzelnen Pixel des Footprints zur endgültigen Pixelfarbe beitragen \cite{rastergrid}.

\begin{figure}[htb]
	\centering
	\includegraphics{images/gaussian_graph}
	\caption[Grafische Darstellung der zweidimensionalen Gauß-Funktion]{Grafische Darstellung der zweidimensionalen Gauß-Funktion\cite{rastergrid}}
	\label{fig:gaussian}
\end{figure}

Dieser Prototyp setzt einen 3x3 Kernel ein. Ein Kernel ist in diesem Anwendungsfall ein quadratisches Array von Pixeln. 
\begin{quote}
	\"{}Jedes Pixel im Bild wird mit dem Gaußschen Kern multipliziert. Dazu wird das mittlere Pixel des Kernels auf dem Bildpixel platziert und die Werte im Originalbild mit den Pixeln im Kernel multipliziert, die sich überlappen. Die aus diesen Multiplikationen resultierenden Werte werden addiert und dieses Ergebnis wird für den Wert am Zielpixel verwendet.\"{}\cite{qastack}. %TODO !! QUELLE FFS
\end{quote}

Je größer der Kernel ist, desto größer ist der Radius der Unschärfe und die Berechnungsdauer. Der $\sigma$-Wert der in Abbildung \ref{eq:gaussian} aufzeigten Gauß'schen Formel bestimmt die Standardabweichung der X- und Y-Richtung. Bei einem Wert von 0 wird die Standardabweichung anhand der Kernelgröße berechnet \cite{opencv_smoothing}. Nach der Minimierung des Rauschens kann anschließend die Hintergrundsubtraktion angewandt werden. 

\vspace*{10mm}
\begin{lstlisting}[caption={Anwendung der Hintergrundsubtraktion}, label={lst:bg_subtraction}]
# Application of background subtraction
fgmask = fgbg.apply(blur_frame)
\end{lstlisting}

Das Ergebnis der Hintergrundsubtraktion ist eine Vordergrundmaske (siehe Abbildung \ref{fig:vordergrundmaske}). Die weiß markierten Stellen stellen den Vordergrund dar, wohingegen die schwarzen Stellen den Hintergrund hervorheben. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/abbildung_fgmask}
	\caption{Erzeugung einer Vordergrundmaske}
	\label{fig:vordergrundmaske}
\end{figure}

\newpage

Nachdem die beweglichen Elemente aus dem Video herauskristallisiert werden können, kann der Video Input für die Objekterkennung vorbereitet werden. Um dieses Ziel erfüllen zu können, wird die Vordergrundsubtraktion eingesetzt. 

\vspace*{10mm}
\begin{lstlisting}[caption={Dilation der Vordergrundmaske}, label={lst:foreground_dilation}]
# Application of foreground dilation
dilated_frame = cv2.dilate(src=fgmask, kernel=np.ones((3,3),np.uint8), iterations=3)
\end{lstlisting}

Eine Erweiterung, auch Dilation genannt, ist Bestandteil der mathematischen Morphologie. Die Morphologie ist eine Untergruppe der Bildverarbeitung. Dabei befasst man sich mit binären Bildern bzw. Rastergrafiken, also Bildern mit nur einem oder zwei Farbwerten \cite{Soille1998}. % TODO: Absolut sinnfreier Abschnitt
Durch die Dilation der Vordergrundmaske werden kleine schwarze Flecken zwischen weißen Flächen verdeckt, sowie die weißen Flächen größer und zu einer Region zusammengefasst. In Abbildung \ref{fig:dilation_foreground_mask} sind bereits größere Strukturen von Objekten zu erkennen, dennoch sind ungewollte, kleinere Flächen sichtbar. Diese werden im nächsten Schritt gefiltert und entfernt. Um die kleineren weißen Flecken zu entfernen werden zunächst alle Konturen der digitalen Vordergrundmaske gesucht. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/abbildung_dilation}
	\caption{Dilation der Vordergrundmaske}
	\label{fig:dilation_foreground_mask}
\end{figure}

\vspace*{10mm}
\begin{lstlisting}[caption={Finden aller Konturen in der Vordergrundmaske}, label={lst:foreground_dilation_detection}]
# Contour detection of foreground mask
contours, _ = cv2.findContours(image=dilated_frame, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)
\end{lstlisting}

Konturen können als eine Kurve erklärt werden, die alle kontinuierlichen Punkte entlang einer Begrenzung verbindet, die die gleiche Farbe oder Intensität besitzen \cite{opencv_contour}. Mit Hilfe der \emph{findContours} Funktion wird eine Liste aller gefundenen Konturen zurückgegeben. Die Kontur ist wiederum eine Liste von Koordinatenpunkten. Der Modus \emph{RETR\_EXTERNAL} gibt nur die extremen Außenkonturen, die eine Schmälerung der gefundenen Knoten mit sich bringt, zurück. Die Methode \emph{CHAIN\_APPROX\_SIMPLE} komprimiert horizontale, vertikale und diagonale Segmente und lässt deren Endpunkte zurück \cite{openstruct}. Um die Konturen auf das Video anzuwenden muss zunächst eine Maske des Videos erstellt werden (siehe Abbildung \ref{lst:mask_creation}).

\vspace*{10mm}
\begin{lstlisting}[caption={Erstellung einer Maske des aktuellen Bildes}, label={lst:mask_creation}]
# Create mask
mask = np.zeros_like(frame)
\end{lstlisting}

Unter Verwendung der \emph{zeroes\_like} Funktion des numpy Packages kann ein Array mit der Form und Größe des Bildes zurückgegeben werden. Dieses Array enthält nur Nullen und ist somit ein schwarzes Bild \cite{numpyzeroeslike}.
Die im Anschluss entstandenen Konturen können nun auf eine vorbestimmte Größe gefiltert werden. Hierfür wird die \emph{contourArea} Funktion verwendet (siehe Abbildung \ref{lst:contourmask}). Mit Hilfe eines Grenzwerts unter 1200 können kleinere, nicht relevante Konturen gefiltert und Objekte wie Personen oder Fahrräder auf der Straße erkannt werden. 

\vspace*{10mm}
\begin{lstlisting}[caption={Erstellen einer Konturenmaske}, label={lst:contourmask}]
# Draw rectangles around contours
for contour in contours:
            
    # Calculate the contour area. Skips small countours
    if cv2.contourArea(contour) < 1200:
        continue
                
    # Returns corners of contour. Doesn't consider the rotation of the object
    (x, y, w, h) = cv2.boundingRect(contour)
                        
    # draw filled contours in mask
    cv2.rectangle(img=mask, pt1=(x, y), pt2=(x+w, y+h), color=(255,255,255), thickness=-1)
\end{lstlisting}

Nach der Filterung der Konturen können die geraden Begrenzungsrechtecke mit Hilfe der \emph{boundingRect} Funktion ermittelt werden. Als gerades Begrenzungsrechteck ist ein Rechteck gemeint, das die Rotation eines Objekts außer Acht lässt. Diese Rechtecke werden als weiße Rechtecke auf die zuvor ermittelte Maske aufgetragen. Der Parameter \emph{thickness} füllt das Rechteck mit der angegebenen Farbe aus, sofern dieser den Wert -1 beinhaltet. Abschließend wird eine bitweise Verrechnung der Maske mit den im Video befindlichen Frames vorgenommen. Durch den $\cap$-Operator werden lediglich die weißen Stellen des Bildes hervorgehoben (siehe Abbildung \ref{fig:abbildungmask} und Auflistung \ref{lst:bitwise_operation}). Um diese Operation anwenden zu können müssen beide Bilder die selbe Größe und Form besitzen.

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/abbildung_mask}
	\caption{Gerade Begrenzungsrechtecke auf der Maske}
	\label{fig:abbildungmask}
\end{figure}

\vspace*{1mm}
\begin{lstlisting}[caption={UND-Operation der Maske und des aktuellen Bildes}, label={lst:bitwise_operation}]
# Bitwise and operation
movement_frame = cv2.bitwise_and(frame, mask)
\end{lstlisting}

Abbildung \ref{fig:bitwiseoperation} zeigt das Ergebnis der Bewegungserkennung. Es werden nur Objekte, die sich im Video bewegen und groß genug für die Erkennung sind, angezeigt. Dies kann anschließend für die Objekterkennung verwendet werden. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/abbildung_movement}
	\caption{Bitweise Operation der Maske und des aktuellen Bildes}
	\label{fig:bitwiseoperation}
\end{figure}

\section{Objekterkennung}\label{ch:objekterkennung}

Um unterschiedliche Verkerhsteilnehmer wie Fahrräder, Autos und Motorräder wahrzunehmen, müssen diese Identifiziert und Erkannt werden. Unter Verwendung der Tensorflow Object Detection \ac{API} wird die Objekterkennung in diesem Prototyp umgesetzt \cite{tf_object_detection_github}. Die Tensorflow Object Detection API ist ein Open-Source Framework, welches auf Tensorflow aufbaut und das Konstruieren, Trainieren und Bereitstellen von Objekterkennungsmodellen erleichtert \cite{tf_object_detection_github}. Alle im Tensorflow Repository enthaltenen Modelle wurden anhand des \ac{COCO} 2017 Datensatzes trainiert. \ac{COCO} ist eine umfangreiche Objekterkennungs-, Segmentierungs- und Beschriftungsdatensatz Bibliothek mit über 330.000 Bildern und 91 Suchkategorien \cite{DBLP:journals/corr/LinMBHPRDZ14}. Diese Modelle sind vor allem für Out-of-the-Box Inferenz nützlich, wenn die gewünschten Kategorien in den Datensätzen vorhanden sind. Objekte wie Fahrräder, Autos, Motorräder, LKWs und Busse sind bereits in den \ac{COCO} Datensätzen enthalten. Dies führt zu dem Anlass, ein bereits vortrainiertes Modell zu verwenden. Jedes Modell wird mit Hilfe der Geschwindigkeit in Millisekunden und der \ac{COCO} \ac{mAP} gemessen. Die Geschwindigkeit gibt an, wie lange ein Modell zur Erkennung von Objekten benötigt. Der \ac{COCO}-\ac{mAP} ist eine weitverbreitete Metrik zur Messung der Genauigkeit eines Modells \cite{jonathanhui}. Je höher der \ac{mAP} Wert, desto genauer ist das Modell. Die Geschwindigkeit und der \ac{mAP} stehen häufig in Relation zueinander. 
Auf Grund dieser Merkmale muss das zu wählende Modell mit den Anforderungen des Projektes zusammenpassen. Für die Echtzeiterkennung wird ein schnelles Modell mit geringer Genauigkeit benötigt. Für diesen Prototypen wurde sich für das Modell mit dem höchsten \ac{COCO}-\ac{mAP} Wert entschieden. Diese Wahl bringt eine durchaus hohe Einbuße der Erkennungsgeschwindigkeit mit sich, welche jedoch auf Grund der Verzichtbarkeit einer Echtzeiterkennung nicht weiter relevant ist. Neben Boxen (Umrandung des Objekts) können Modelle ebenfalls Keypoints zurückgeben, die für die weitere Kompatibilität relevant ist. Bei der Keypoint Erkennung werden Personen erkannt und gleichzeitig deren Keypoint lokalisiert. Keypoints sind räumliche Orte bzw. Punkte im Bild, die definieren, was interessant ist oder im Bild hervorsticht. Sie sind invariant gegenüber Bilddrehung, -Schrumpfung, -Verschiebung und -Verzerrung \cite{kp_detection}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/keypoint-detection}
	\caption[Keypoint Erkennung]{Keypoint Erkennung \footnotemark}
	\label{fig:keypoint-detection}
\end{figure}
\footnotetext{https://raw.githubusercontent.com/openpifpaf/openpifpaf/main/docs/coco/000000081988.jpg}

Für diesen Prototypen wird das EfficientDet-D7 Modell verwendet, das mit einem \ac{COCO}-\ac{mAP} Wert von 51,2 im Tensorflow Model Zoo ideal dazu geeignet ist, um die markantesten Objekte in einem Bild zu lokalisieren \cite{efficientdet}. EfficientDet ist die Objekterkennungs Variante des weitverbreiteten EfficientNet \cite{tan2019efficientnet} und baut auf dessen Erfolg auf. EfficientNets stammen aus einer Familie von Modellen, die eine hohe Leistung bei Benchmark-Aufgaben erreichen, während sie für eine Reihe von Effizienzparametern wie Modellgröße und \acp{FLOP} kontrolliert werden. Das Netzwerk wird in einer Reihe von Modellgrößen d0-d7 ausgeliefert. \acp{FLOP} bemisst die Anzahl an Gleitkommaoperationen pro Sekunde und ist ein Maß für die Leistungsfähigkeit eines Modells \cite{object_detection_comparison}. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{images/efficientdet}
	\caption[Modell FLOPs vs COCO Genauigkeit]{Modell \acp{FLOP} vs. \ac{COCO} Genauigkeit \cite{tan2020efficientdet}}
\end{figure}

EfficientDet erreicht die beste Leistung, mit der geringsten Anzahl von Trainingsepochen, unter den Objekterkennungsmodellen. Dies ist insbesondere von Vorteil, wenn mit begrenzter Rechenleistung gearbeitet wird \cite{solawetz}. Bevor das vortrainierte Modell verwendet werden kann, wird dieses als verpackte Datei heruntergeladen. Ebenfalls enthalten ist die zugehörige Beschreibungsdatei, welche eine Liste von Strings beinhaltet, die zur richtigen Beschriftung eines Objekts dient. Jede Beschriftung ist durch eine ID, die später zur Visualisierung verwendet wird, eindeutig erkennbar. Nach dem Herunterladen des Modells wird dieses extrahiert und mit der Beschreibungsdatei geladen (siehe Auflistung \ref{lst:loading})\cite{obd_api}.

\vspace*{10mm}
\begin{lstlisting}[caption={Laden der Beschreibungsdatei mit anschließendem Erzeugen des Modells}, label={lst:loading}]
category_index = label_map_util.create_category_index_from_labelmap(use_display_name=True)
# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)
\end{lstlisting}

Das Modell enthält zusätzlich eine Pipeline Konfigurationsdatei, welche zur Erzeugung des Modells verwendet wird. Eine Pipeline definiert Konfigurationen für ein Modell und automatisiert dadurch den Workflow des zu bauenden Modells. Pipelines für maschinelles Lernen bestehen aus mehreren aufeinanderfolgenden Schritten, die von der Datenextraktion und Vorverarbeitung bis hin zum Modelltraining und der Bereitstellung reichen \cite{valohai}.

\vspace*{10mm}
\begin{lstlisting}[caption={Laden der Checkpoints}, label={lst:checkpoints}]
# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()
\end{lstlisting}

Checkpoints erfassen den genauen Wert aller von einem Modell verwendeten Parameter \cite{tf_checkpoint}. Sodurch kann ein Modell wiederhergestellt und verwendet werden. Um nun eine Objekterkennung durchführen zu können, erwartet das EfficientDet-D7 Modell ein Dreikanalbild variabler Größe. Die Eingabewerte benötigen einen Tensor mit einer Höhe von 1 und einer Breite von 3 und zugehörigen Werten zwischen 0 und 255 \cite{efficientdet}. Auf Grund dieser Anforderung wird das Bild mit Hilfe der \gls{Numpy} Funktion \emph{expand\_dims} auf die erwartete Größe skaliert und in einen Tensor umgewandelt. Anschließend kann die Objekterkennung ausgeführt werden.

\vspace*{10mm}
\begin{lstlisting}[caption={Anpassung des Eingabetensors an das Modell}, label={lst:tensorfit}]
# Expand dimensions since the model expects images to have shape: [1, None, None, 3]
image_np_expanded = np.expand_dims(frame, axis=0)
input_tensor = tf.convert_to_tensor(image_np_expanded, dtype=tf.float32)
\end{lstlisting}

\begin{lstlisting}[caption={Ausführung der Objekterkennung}, label={lst:objectdetection_execution}]
# Execute object detection
detections, predictions_dict, shapes = detect_fn(input_tensor)
\end{lstlisting}

Die Funktion \emph{detect\_fn} gibt alle notwendigen Angaben als Python Dictionary zurück. Für den Prototypen werden die \emph{detection\_boxes}, \emph{detection\_scores} und \emph{detection\_classes} benötigt. \emph{Detection\_boxes} ist ein Tensor welcher die Koordinaten für 100 minimal umgebene Rechtecke, sogenannte Bounding Boxes, beinhaltet. Anhand dieser Koordinaten kann im späteren Verlauf der Objekterekennung die Umrandung der Objekte gezeichnet werden. 
 
\vspace*{10mm}
\begin{lstlisting}[caption={Beispielausschnitt der detection boxes}, label={lst:detectionbox_sample}]
'detection_boxes': <tf.Tensor: shape=(1, 100, 4), dtype=float32, numpy=
 array([[[0.4823483 , 0.33256257, 0.8408261 , 0.6648078 ],
         [0.39217225, 0.3756986 , 0.5196357 , 0.50812906],
         [0.482716  , 0.3359049 , 0.84076875, 0.6666065 ],
         [0.6159764 , 0.5550573 , 0.6771329 , 0.59031546],
         [0.39108938, 0.39384803, 0.43105263, 0.4248545 ]
\end{lstlisting}

Das zugehörige Objekt kann anhand des Indizes in der \emph{detection\_class} gefunden werden.

\vspace*{5mm}
\begin{lstlisting}[caption={Beispielausschnitt der detection classes}, label={lst:detectionclasses_sample}]
'detection_classes': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=
 array([[ 2,  2,  7,  0,  0,  0,  2,  2,  7,  7,  2,  7,  2,  7,  2,  2,
          0, 26,  7,  5,  2,  7,  0,  2,  7,  0,  2, 30,  0,  0,  2,  2,
          0,  2,  5,  7,  2,  0, 36,  7,  7,  0,  2,  0,  9,  7, 13,  3,
         76,  7, 36, 61,  2,  2, 61,  7, 26, 61,  7,  2, 32, 13, 13,  0,
         17,  0, 32,  5,  5, 76, 27,  7,  7, 32, 26,  2,  7,  2,  9, 26,
         27,  0,  7, 13, 32, 33,  3,  2, 63, 43, 30, 13, 43,  1,  7, 13,
         30,  9,  2,  7]], dtype=int32)>,
\end{lstlisting}

Jede Zahl in dem \emph{detection\_classes} Tensor ist eine ID der in Auflistung \ref{lst:loading} geladenen Beschreibungsdatei. Ebenso kann anhand des Indizes die Genauigkeit der Erkennung in dem \emph{detection\_scores} Objekt abgefragt werden (siehe Auflistung \ref{lst:detectionscores_sample}).

\vspace*{5mm}
\begin{lstlisting}[caption={Beispielausschnitt der detection scores}, label={lst:detectionscores_sample}]
'detection_scores': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=
 array([[0.65809727, 0.49682707, 0.32241914, 0.191383  , 0.16080539,
\end{lstlisting}

Das Dictionary \emph{detections} enthält weitere Ausgabe Parameter, die für diesen Prototypen jedoch irrelevant sind \cite{efficientdet}. Nicht alle Objekte die vom Modell erkannt werden, sind für die Objekterkennung notwendig. Daher werden die erkannten Objekte anhand der Klassen-ID gefiltert und aussortiert. Somit erhält man die Umrandungen, Klassen und Genauigkeiten eines einzelnen Bildes für die gewünschten Klassen. Die Visualisierung der Daten erfolgt in einem späteren Schritt. 
\newpage
\section{Geschwindigkeits- und Fahrtrichtungserkennung}

Damit Geschwindigkeit und Fahrtrichtung mehrerer Objekte gleichzeitig gemessen werden kann, müssen die Objekte über die Frames hinaus getrackt werden. Die Objekterkennung bietet zwar die Möglichkeit, Objekte eines einzelnen Bildes zu erkennen, kann jedoch die Objekte des nächsten Frames nicht mit den vorherigen in Verbindung bringen. Auf Grund dieser Tatsache muss ein Tracking Algorithmus verwendet werden, der Objekte über verschiedene Bilder identifizieren kann. In diesem Prototypen wird das Verfolgen der Objekte mit Hilfe des \acs{SORT} Algorithmus durchgeführt. \ac{SORT} ist eine Barebone Implementierung einer visuellen Mehrfach-Objektverfolgung, die auf rudimentären Datenassoziations- und Zustandsschätzungs-Techniken basiert \cites{sort_gh}{DBLP:journals/corr/BewleyGORU16}. Es ist für Online-Tracking Anwendungen konzipiert, bei denen nur vergangene und aktuelle Frames zur Verfügung stehen \cite{sort_gh}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{images/sort}
	\caption[Performance Benchmark unterschiedlicher Tracking Algorithmen]{Performance Benchmark unterschiedlicher Tracking Algorithmen \cite{DBLP:journals/corr/BewleyGORU16}}
	\label{fig:sort}
\end{figure}

Bei der Veröffentlichung am 7. Juli 2017 gehört der \ac{SORT} Algorithmus zu den besten Open-Source Trackern \cite{DBLP:journals/corr/BewleyGORU16} (siehe Abbildung \ref{fig:sort}). Minimalistische Tracker behandeln keine Verdeckung oder wieder eintretende Objekte und dienen als Basis und Testumgebung für die Entwicklung zukünftiger Tracker. Jedoch muss beachtet werden, dass die Genauigkeit des Trackers stark von der Objekterkennung abhängig ist \cite{sort_gh}. Trotz alledem ist der \ac{SORT} Algorithmus, dank der simplen Einbindung und der Leistungsfähigkeit, ein gutes Werkzeug um Objekte im Prototypen zu verfolgen. \ac{SORT} verwendet zwei verbreitete Techniken um Objekte zu verfolgen: den Kalman Filter und die ungarische Methode. 

\begin{quote}
	\"{}Der Kalman Filter findet einen Schätzer eines dynamischen Systems anhand von vorherigen Messungen und einem Systemmodell, welches das zu messende System simuliert. Der Kalman Filter benutzt die Information, welche ihm das vorherige Zeitintervall liefert, um eine a-priori Vorhersage zu treffen. Diese wird dann anhand einer neuen Messung korrigiert\"{} \cite{asida_kalman}.
\end{quote}

Die Ungarische Methode ist dazu gedacht, lineare Zuordnungsprobleme zu lösen \cite{hungarian_method}. Beide Techniken werden verwendet, um die Komponenten der Bewegungsvorhersage bzw. der Datenzuordnung des Verfolgungsproblems zu behandeln \cite{DBLP:journals/corr/BewleyGORU16}.

\vspace*{5mm}
\begin{lstlisting}[caption={Verwendung des SORT Algorithmus}, label={lst:sort}]
from sort import *
mot_tracker = Sort()

# get detections for each frame
...
# update SORT each frame
track_bbs_ids = mot_tracker.update(filtered_detections['detection_boxes'][0])

# track_bbs_ids is a np array where each row contains a valid bounding box and track_id
...
\end{lstlisting}

Um den \ac{SORT} Algorithmus zu verwenden, muss die \emph{sort.py} Datei aus dem offiziellen Repository heruntergeladen und anschließend importiert werden \cite{sort_gh}. Für jeden Frame eines Videos, muss zuerst die Objekterkennung ausgeführt, sowie die aus Kapitel \ref{ch:objekterkennung} erhaltenen Objekte gefiltert werden. Die gefilterten Objekte können darauf folgend an den Algorithmus übergeben werden. In dem \gls{Numpy} Array \emph{track\_bbs\_ids} befinden sich Bounding Box Koordinaten, sowie eine zugehörige Track-ID für jedes verfolgte Objekt. Mit Hilfe von Koordinaten und Track-ID kann die Objekterkennung über mehrere Bilder hinweg realisiert werden. Für die Fahrtrichtungserkennung werden die letzten drei X Koordinaten des Mittelpunkts einer Bounding-Box verwendet. Gespeichert werden diese in einem temporären Python Dictionary, wo über die Track-ID auf die Koordinaten zugegriffen werden kann. In diesem Prototypen werden lediglich die X Koordinaten gespeichert, da sich die Objekte nur auf der horizontalen Achse bewegen können.

\vspace*{5mm}
\begin{lstlisting}[caption={Temporäres Speichern der X Koordinaten eines verfolgten Objekts}, label={lst:tempsave}]
# add x coordinates of tracked object to track_id
if track_id in direction_tracking:
    direction_tracking[track_id].append(x_coord)
else:
    direction_tracking[track_id] = [x_coord]
\end{lstlisting}

In jedem Frame wird abgefragt, ob die Track-ID bereits vorhanden ist und, ob bereits mehr als drei Koordinaten gespeichert wurden. Ist dies der Fall, werden die letzten drei Koordinaten verglichen. 

\vspace*{5mm}
\begin{lstlisting}[caption={Fahrtrichtungserkennung eines Objektes nach Rechts}, label={lst:directiondetection}]
if locations[-1] > (locations[-2] and  locations[-3]):
    directions[track_id] = 'right'
...
\end{lstlisting}

Ist die zuletzt erhaltene Koordinate eines Objektes größer als die zwei vorletzten, so wird die Richtung als Rechts gespeichert. Ist die erhaltene Koordinate kleiner als die zwei vorletzten, bewegt sich das Objekt nach links. Auch hier wird die Richtung einer Track-ID in einem temporären Dictionary gespeichert. Bei der Geschwindigkeitsberechnung wird für diesen Prototypen ein Bereich festgelegt, in dem die Bemessung ausgeführt wird. Mit Hilfe dieses Bereichs kann eine Approximation der Geschwindigkeit getroffen werden. Darüber hinaus ist die Schwankung der gemessenen Geschwindigkeit geringer, da nur ein kleiner Bereich gemessen wird. Objekte die zum Stillstand kamen oder durch fehlerhafte Objekterkennung nicht mehr verfolgt werden, sind von der Messung ausgeschlossen. Die Reichweite wurde bei diesem Prototyp auf eine Spanne von ca. 4,5 Metern, in der Mitte des Bildes, eingestellt. 

\newpage

\vspace*{5mm}
\begin{lstlisting}[caption={Berechnung der Geschwindigkeit}, label={lst:speedmeasurement}]
# Only objects that will drive completely through the speed detection area will be calculated
if (locations[-1] >= 452 and min(locations) < 308) or (locations[-1] <= 308 and max(locations) > 452):
        # get amount of all x coordinates in the speed detection area
        speed_tracking[track_id] = len([mark for mark in locations if (mark >= 308 and <=452)])
                                
        if speed_tracking[track_id] != 0:
            # speed = 4.5 meters / (amount of x coordinates in area / fps) * 3.6
            speed = 4.5/(speed_tracking[track_id]/FPS)*3.6
\end{lstlisting}

Ein Objekt, das je nach Richtung bereits vor dem Bereich als verfolgtes Objekt erkannt wurde und über den gesamten Geschwindigkeitsbereich verfolgt wurde, wird zur Geschwindigkeitsberechnung herangezogen. Anschließend werden die Frames gezählt, wie lange sich das Objekt in dem Bereich befand. Dies wird anhand der Anzahl der X-Koordinaten gemessen, die sich im Bereich befinden. Die Berechnung der Geschwindigkeit erfolgt mit der folgenden Formel:

\begin{equation}
	\text{Geschwindigkeit in km/h} = \frac{4,5m}{\frac{\text{Anzahl der Frames im Geschwindigkeitsbereich}}{\text{FPS Anzahl des Videos}} * 3,6} 
\end{equation}

Das bedeutet, dass für die Geschwindigkeitsberechnung eine vollständige Objektverfolgung und damit auch eine Objekterkennung stattfinden muss. Ebenso gilt, je höher die FPS-Zahl des Videos, desto genauer ist die Berechnung.

\section{Visualisierung und Erhebung von Statistiken}

Für die Visualisierung der gemessenen und erkannten Metriken wie Fahrtrichtung und Objekterkennung, muss die Objekterkennung mit den verfolgten Objekten des \ac{SORT} Algorithmus verknüpft werden. Das liegt daran, dass die Rechtecke der Objekterkennung nicht mit den vom Tracking-Algorithmus erhaltenen Rechtecken identisch ist. Der Tracking-Algorithmus verwendet die Daten der Objekterkennung auf Basis der erhaltenen Daten des SORT-Algorithmus, jedoch kann nicht auf das Objekt der Objekterkennung zugegriffen werden. Um das erkannte Ziel der Objekterkennung den richtigen Geschwindigkeits- und Fahrtrichtungswerten zuzuordnen, werden die Daten auf Basis der \glspl{BoundingBox} der Objekterkennung und des Tracking-Algorithmus verknüpft. Dazu wird die Überlappungsfläche aller \glspl{BoundingBox} der Objekterkennung mit denen des Tracking-Algorithmus verglichen. Die Rechtecke mit der größten Fläche werden als das gleiche Objekt betrachtet.

\vspace*{5mm}
\begin{lstlisting}[caption={Berechnung der Fläche zweier Rechtecke a und b}, label={lst:computation_of_area}]
dx = min(axmax, bxmax) - max(axmin, bxmin)
dy = min(aymax, bymax) - max(aymin, bymin)

if (dx>=0) and (dy>=0):
    return dx*dy
else:
    return 0
\end{lstlisting}

Da nun die Objekte des Tracking-Algorithmus und der Objekterkennung bekannt sind, können die Daten gemeinsam visualisiert und gespeichert werden. Für die Visualisierung eines Rechteckes um das Objekt, wird die \gls{BoundingBox} des \ac{SORT} Algorithmus verwendet, da diese oft genauer und länger vorhanden ist. Die Werte der Objekterkennung, wie Objektklasse und -genauigkeit, werden oberhalb des Rechteckes dargestellt. Die Fahrtrichtung wird anhand eines Pfeils links bzw. rechts des Rechteckes angezeigt. Sobald eine Geschwindigkeit für ein Objekt gemessen wurde, wird diese unterhalb des Objekts angezeigt (siehe Abbildung \ref{fig:visual}).

\begin{figure}[htb]
	\centering
	\includegraphics[width=.9\textwidth]{abbildung_final}
	\caption{Visualisierung der Objekterkennungs- und Trackingdaten}
	\label{fig:visual}
\end{figure}

Der Bereich der Geschwindigkeitsmessung wird durch zwei vertikale blaue Linien visualisiert. Befindet sich ein Objekt innerhalb dieses Bereichs, wird das Rechteck des Objekts rot gefärbt. Auf Grund der Anforderungen an Statistiken über das Verkehrsaufkommen, wird sobald die Geschwindigkeit eines Objekts gemessen wurde, die Anzahl der zugehörigen Objektklasse um eins erhöht. Visualisiert wird dies im unteren linken Eck des Videos. 

Um weitere Ableitungen der Statistiken zu ermöglichen, werden die Daten mit Hilfe des Python Moduls \emph{XlsXWriter}\footnote{https://pypi.org/project/XlsxWriter/} gesammelt und am Ende der Programmausführung in einer Excel Datei gespeichert. Die Excel Dateien werden im \emph{statistics}-Ordner abgespeichert. Eine Zuordnung der Dateien wird durch eine Namenskonvention ermöglicht, die Uhrzeit und Datum der Erstellung verwendet. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=.6\textwidth]{excelsaving}
	\caption{Auszug der Beispielstatistiken}
	\label{fig:excelsaving}
\end{figure}

Zur Speicherung eines jeden gemessenen Objekts werden \emph{track\_id}, eine interne ID des \ac{SORT} Algorithmus zur Identifikation des eigentlichen Objekts, sowie die Daten der Objekterkennung, die Fahrtrichtung und Geschwindigkeit anzeigen, gesichert. Des weiteren wird eine Summierung der erkannten Klassen gespeichert. Die erhobenen Daten können zu weiteren Ableitungen des Verkehrsaufkommens verwendet werden. 

Bildausschnitte, die die gemessenen Objekte des Prototypen darstellen, werden im \emph{objects}-Ordner abgespeichert (siehe Auflistung \ref{lst:save}). Mit Hilfe der \emph{track\_id} wird eine Identifikation dieser Objekte ermöglicht. Die durch den Prototypen modifizierten Videosequenzen werden im \emph{output\_videos}-Ordner gesichert.

\vspace*{5mm}
\begin{lstlisting}[caption={Speichern eines Bildausschnittes}, label={lst:save}]
cv2.imwrite('objects/object_{}.jpg'.format(track_id), movement_frame[y1:y2, x1:x2])
\end{lstlisting}




